# -*- coding: utf-8 -*-
"""LVADSUSR151_dheeraj_lab_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JdHD7ZwWqlOTJnzFf7tS7aN8eC_0AjK1
"""

#Importing the Libraries
import numpy as np
import pandas as pd
import datetime
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt, numpy as np
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import AgglomerativeClustering
from matplotlib.colors import ListedColormap
from sklearn import metrics

data = pd.read_csv('/content/customer_segmentation (1).csv')

data.head()





#Information on features
data.info()

from sklearn.impute import SimpleImputer
import numpy as np

# Define a custom imputer function
def custom_imputer(column):
    if column.skew() > 1 or column.skew() < -1:
        return column.median()
    else:
        return column.mean()

# Apply custom imputer to each column
def impute_with_custom(df):
    for col in df.columns:
        if df[col].dtype in ['int64', 'float64']:
            df[col].fillna(custom_imputer(df[col]), inplace=True)
        else:
            df[col].fillna(df[col].mode()[0], inplace=True)
    return df

# Impute using the custom imputer function
wine_imputed = impute_with_custom(data)

# Check if there are any missing values left
print(wine_imputed.isnull().sum())

data.info()

data.describe()

import pandas as pd
import numpy as np

# Define a function to remove outliers using Z-score
def remove_outliers_zscore(data, threshold=3):
    z_scores = (data - data.mean()) / data.std()
    filtered_data = data[(np.abs(z_scores) < threshold).all(axis=1)]
    return filtered_data

# Assuming 'data' is your DataFrame with numeric columns
# Apply Z-score outlier removal for numeric columns
numeric_data = data.select_dtypes(include=np.number)
filtered_data = remove_outliers_zscore(numeric_data)

print("The total number of data-points after removing the outliers is:", len(filtered_data))

numeric_data = data.select_dtypes(include=np.number)

corrmat = numeric_data.corr()

# Plot the correlation matrix
plt.figure(figsize=(20, 20))
sns.heatmap(corrmat, annot=True, cmap='coolwarm', center=0,linewidths= 1)
plt.show()

def select_features(data, threshold=0.5):
    # Exclude non-numeric columns
    numeric_data = data.select_dtypes(include=np.number)

    # Calculate correlation matrix
    corrmat = numeric_data.corr()

    # Filter features based on correlation threshold
    selected_features = set()
    for i in range(len(corrmat.columns)):
        for j in range(i):
            if abs(corrmat.iloc[i, j]) > threshold:
                col_i = corrmat.columns[i]
                col_j = corrmat.columns[j]
                # Add both features to the selected features set
                selected_features.add(col_i)
                selected_features.add(col_j)

    return selected_features

# Automatically select features based on a correlation threshold
selected_features = select_features(data, threshold=0.5)
print("Selected features based on correlation threshold:", selected_features)

#Get list of categorical variables
s = (data.dtypes == 'object')
object_cols = list(s[s].index)

print("Categorical variables in the dataset:", object_cols)

#Label Encoding the object dtypes.
LE=LabelEncoder()
for i in object_cols:
    data[i]=data[[i]].apply(LE.fit_transform)

print("All features are now numerical")

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Define X (features)
X = data.copy()  # Assuming all columns are features

# Standardize the features
scaler = StandardScaler()
scaled_X = scaler.fit_transform(X)

# Apply PCA
pca = PCA(n_components=3)
pca.fit(scaled_X)

# Determine the number of components to keep based on explained variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)
num_components = np.argmax(cumulative_explained_variance_ratio >= 0.95) + 1  # Keep components explaining at least 95% of variance

# Fit PCA with the selected number of components
pca = PCA(n_components=num_components)
selected_features = pca.fit_transform(scaled_X)

# Create a DataFrame with the transformed features
transformed_df = pd.DataFrame(selected_features, columns=[f'PC{i}' for i in range(1, num_components + 1)])

print("Transformed features after PCA:")
transformed_df.head()

data.head()

s = (data.dtypes == 'object')
object_cols = list(s[s].index)

# Label Encoding the object dtypes.
LE = LabelEncoder()
for i in object_cols:
    data[i] = data[[i]].apply(LE.fit_transform)

data.head()

X = data.iloc[:, 0:-1].values  # Select all columns except the last one as features
y = data.iloc[:, -1].values  # Select the last column as the target variable

# Split the dataset into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Scale the features
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

import matplotlib.pyplot as plt
# Initialize the KMeans model with the desired number of clusters
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)

# Fit the model to the data
y_pred = kmeans.fit_predict(X)

# Print the cluster labels
print(y_pred)

# Visualize the clusters
plt.figure(figsize=(10, 7))
plt.scatter(X[y_pred == 0, 0], X[y_pred == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X[y_pred == 2, 0], X[y_pred == 2, 1], s=100, c='green', label='Cluster 3')

# Plot the centroids
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')

# Add labels and title
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

from sklearn.metrics import silhouette_score

# Calculate silhouette score for KMeans with 3 clusters
silhouette_score_kmeans_3 = silhouette_score(X, kmeans.labels_, metric='euclidean')

# Print the silhouette score
print("Silhouette score for KMeans with 3 clusters:", silhouette_score_kmeans_3)